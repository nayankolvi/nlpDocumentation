{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constituency parsing is a natural language processing task which involves parsing the sentences in a text into their constituent phrases or \"constituents\", following the formalism of context-free grammars. It is used in Stanza through a shift-reduce parser, which is a type of parser that iteratively shifts input onto a stack and reduces it to construct the parse tree stanfordnlp.github.io.\n",
    "\n",
    "The ConstituencyProcessor in Stanza adds a constituency parse tree to each Sentence in the text. Bracket types depend on the treebank used, but custom models can support any set of labels as long as there is training data stanfordnlp.github.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kolvi\\AppData\\Local\\anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# config = {\n",
    "#     # Comma-separated list of processors to use\n",
    "# \t'processors': 'tokenize,mwt,pos',\n",
    "#     # Language code for the language to build the Pipeline in\n",
    "#     'lang': 'fr',\n",
    "#     # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "#     # You only need model paths if you have a specific model outside of stanza_resources\n",
    "# \t# 'tokenize_model_path': './fr_gsd_models/fr_gsd_tokenizer.pt',\n",
    "# \t# 'mwt_model_path': './fr_gsd_models/fr_gsd_mwt_expander.pt',\n",
    "# \t# 'pos_model_path': './fr_gsd_models/fr_gsd_tagger.pt',\n",
    "# \t# 'pos_pretrain_path': './fr_gsd_models/fr_gsd.pretrain.pt',\n",
    "#     # Use pretokenized text as input and disable tokenization\n",
    "# \t# 'tokenize_pretokenized': True\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentenceStruct:\n",
    "    def __init__(self):\n",
    "        self.condition = \"\"\n",
    "        self.system = \"\"\n",
    "        self.event = \"\"\n",
    "        self.transition = \"\"\n",
    "        self.action = \"\"\n",
    "        self.state = \"\"\n",
    "\n",
    "class SentenceSeg():\n",
    "    def __init__(self):\n",
    "        self.preposition = \"\"\n",
    "        self.verb = \"\"\n",
    "        self.NounPhrase = \"\"\n",
    "\n",
    "    def isEmpty(self):\n",
    "        if self.preposition == \"\" and self.verb == \"\" and self.NounPhrase == \"\":\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBuild():\n",
    "    def __init__(self):\n",
    "        self.segment = SentenceSeg()\n",
    "        self.sentence = sentenceStruct()\n",
    "        self.segments = []\n",
    "        self.sentences = []\n",
    "\n",
    "        self.states = [\"store\", \"process\"]\n",
    "        self.state = \"\"\n",
    "\n",
    "        self.tempData = \"\"\n",
    "        self.data = []\n",
    "\n",
    "    def process(self, depRel = None, val = None):\n",
    "        if depRel == \"VP\":\n",
    "            # if self.segment.isEmpty():\n",
    "            #     self.segment = SentenceSeg()\n",
    "            # else:\n",
    "            #     pass\n",
    "            self.state = \"store\"\n",
    "            self.data.append(self.tempData.strip())\n",
    "            self.tempData = \"\"\n",
    "            \n",
    "\n",
    "        elif val != None:\n",
    "            self.state = \"process\"\n",
    "            self.tempData += val + \" \"\n",
    "\n",
    "        elif depRel == \"End\" and val == None:\n",
    "            self.state = \"store\"\n",
    "            self.data.append(self.tempData.strip())\n",
    "            self.tempData = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tree datastructure\n",
    "\"\"\"\n",
    "\n",
    "from collections import deque, Counter\n",
    "from enum import Enum\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# useful more for the \"is\" functionality than the time savings\n",
    "CLOSE_PAREN = ')'\n",
    "SPACE_SEPARATOR = ' '\n",
    "OPEN_PAREN = '('\n",
    "\n",
    "EMPTY_CHILDREN = ()\n",
    "\n",
    "CONSTITUENT_SPLIT = re.compile(\"[-=#]\")\n",
    "\n",
    "# These words occur in the VLSP dataset.\n",
    "# The documentation claims there might be *O*, although those don't\n",
    "# seem to exist in practice\n",
    "WORDS_TO_PRUNE = ('*E*', '*T*', '*O*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(tree, normalize=None):\n",
    "        \"\"\"\n",
    "        Print with newlines & indentation on each line\n",
    "\n",
    "        \n",
    "        Preterminals and nodes with all preterminal children go on their own line\n",
    "\n",
    "        You can pass in your own normalize() function.  If you do,\n",
    "        make sure the function updates the parens to be something\n",
    "        other than () or the brackets will be broken\n",
    "        \"\"\"\n",
    "        if normalize is None:\n",
    "            normalize = lambda x: x.replace(\"(\", \"-LRB-\").replace(\")\", \"-RRB-\")\n",
    "\n",
    "        # val = []\n",
    "        # arr = []\n",
    "\n",
    "        sent = SentenceBuild()\n",
    "\n",
    "        indent = 0\n",
    "        with StringIO() as buf:\n",
    "            stack = deque()\n",
    "            stack.append(tree)\n",
    "            while len(stack) > 0:\n",
    "                node = stack.pop()\n",
    "                \n",
    "                # if val != []:\n",
    "                #     tempData = \"\"\n",
    "                #     for elem in val:\n",
    "                #         tempData += elem + \" \"\n",
    "                #     arr.append(tempData.strip())\n",
    "                #     val = []\n",
    "\n",
    "                if node is CLOSE_PAREN:\n",
    "                    # if we're trying to pretty print trees, pop all off close parens\n",
    "                    # then write a newline\n",
    "                    while node is CLOSE_PAREN:\n",
    "                        indent -= 1\n",
    "                        buf.write(CLOSE_PAREN)\n",
    "                        if len(stack) == 0:\n",
    "                            node = None\n",
    "                            break\n",
    "                        node = stack.pop()\n",
    "                    buf.write(\"\\n\")\n",
    "                    if node is None:\n",
    "                        break\n",
    "                    stack.append(node)\n",
    "                elif node.is_preterminal():\n",
    "                    buf.write(\"  \" * indent)\n",
    "                    ####\n",
    "                    sent.process(node.label, node.children[0].label)\n",
    "                    ####\n",
    "                    buf.write(\"%s%s %s%s\" % (OPEN_PAREN, normalize(node.label), normalize(node.children[0].label), CLOSE_PAREN))\n",
    "                    if len(stack) == 0 or stack[-1] is not CLOSE_PAREN:\n",
    "                        buf.write(\"\\n\")\n",
    "                elif all(x.is_preterminal() for x in node.children):\n",
    "                    buf.write(\"  \" * indent)\n",
    "                    buf.write(\"%s%s\" % (OPEN_PAREN, normalize(node.label)))\n",
    "                    for child in node.children:\n",
    "                        ####\n",
    "                        sent.process(child.label, child.children[0].label)\n",
    "                        ####\n",
    "                        buf.write(\" %s%s %s%s\" % (OPEN_PAREN, normalize(child.label), normalize(child.children[0].label), CLOSE_PAREN))\n",
    "                    buf.write(CLOSE_PAREN)\n",
    "                    if len(stack) == 0 or stack[-1] is not CLOSE_PAREN:\n",
    "                        buf.write(\"\\n\")\n",
    "                else:\n",
    "                    buf.write(\"  \" * indent)\n",
    "                    buf.write(\"%s%s\\n\" % (OPEN_PAREN, normalize(node.label)))\n",
    "                    ####\n",
    "                    sent.process(node.label, None)\n",
    "                    ####\n",
    "                    stack.append(CLOSE_PAREN)\n",
    "                    for child in reversed(node.children):\n",
    "                        stack.append(child)\n",
    "                    indent += 1\n",
    "\n",
    "            buf.seek(0)\n",
    "            ####\n",
    "            sent.process(\"End\", None)\n",
    "            ####\n",
    "            return buf.read(), sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 15:12:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 4.42MB/s]                    \n",
      "2023-09-06 15:12:09 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-09-06 15:12:09 WARNING: Can not find ner: aerobert-ner from official model list. Ignoring it.\n",
      "2023-09-06 15:12:10 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| lemma        | combined |\n",
      "| constituency | wsj_bert |\n",
      "| depparse     | combined |\n",
      "| sentiment    | sstplus  |\n",
      "| ner          | conll03  |\n",
      "===========================\n",
      "\n",
      "2023-09-06 15:12:10 INFO: Using device: cpu\n",
      "2023-09-06 15:12:10 INFO: Loading: tokenize\n",
      "2023-09-06 15:12:10 INFO: Loading: pos\n",
      "2023-09-06 15:12:10 INFO: Loading: lemma\n",
      "2023-09-06 15:12:10 INFO: Loading: constituency\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-09-06 15:12:16 INFO: Loading: depparse\n",
      "2023-09-06 15:12:16 INFO: Loading: sentiment\n",
      "2023-09-06 15:12:17 INFO: Loading: ner\n",
      "2023-09-06 15:12:17 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "processor_config = {\n",
    "    'tokenize': 'default',\n",
    "    'mwt': 'default',\n",
    "    'pos' : 'default',\n",
    "    'lemma' : 'default',\n",
    "    'depparse' : 'default',\n",
    "    'constituency' : 'wsj_bert',\n",
    "    'ner' : [\"CoNLL03\", \"aeroBERT-NER \"]\n",
    "}\n",
    "\n",
    "# https://www.researchgate.net/publication/371428620_SafeAeroBERT_Towards_a_Safety-Informed_Aerospace-Specific_Language_Model?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ\n",
    "\n",
    "# sentence tockenizers\n",
    "sentence_tokenize = stanza.Pipeline(lang='en', processors=processor_config)\n",
    "# sentence_tokenize_no_split = stanza.Pipeline(lang='en', processors={'tokenize': 'spacy'}, tokenize_no_ssplit=True)\n",
    "# sentence_preTokenised = stanza.Pipeline(lang='en', processors={'tokenize': 'spacy'}, tokenize_pretokenized=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentTockenize(sentence):\n",
    "    tokens = sentence_tokenize(sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP\n",
      "      (NP (DT The) (NN transition))\n",
      "      (PP\n",
      "        (IN from)\n",
      "        (NP (NNP CSWS) (RB OFF)))\n",
      "      (PP\n",
      "        (IN to)\n",
      "        (NP (NNP CSWS) (IN ON))))\n",
      "    (VP\n",
      "      (MD can)\n",
      "      (VP\n",
      "        (VB be)\n",
      "        (VP\n",
      "          (VBN performed)\n",
      "          (PP\n",
      "            (PP\n",
      "              (IN by)\n",
      "              (NP (DT the) (NN driver)))\n",
      "            (CC or)\n",
      "            (ADVP (RB automatically))))))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The transition from CSWS OFF to CSWS ON can be performed by the driver or automatically\"\n",
    "\n",
    "doc = sentTockenize(sentence)\n",
    "tree = doc.sentences[0].constituency\n",
    "\n",
    "out, array = pretty_print(tree)\n",
    "print(out)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
