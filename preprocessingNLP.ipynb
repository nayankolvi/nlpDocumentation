{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kolvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# Download stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordTockenize(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "\n",
    "def removePunctandStopWords(tokens):\n",
    "    toks = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.lower() not in stopwords.words('english'):\n",
    "            if token not in punct:\n",
    "                toks.append(token)\n",
    "\n",
    "    return toks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stem(tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = stemmer.stem(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('string', 3), ('token', 1), ('must', 2), ('stem', 1), ('punctuat', 1)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_freqs(tokens):\n",
    "    # Build from tockenized preprocessed words\n",
    "    freqs = defaultdict()\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in freqs:\n",
    "            freqs[word] += 1\n",
    "        else:\n",
    "            freqs[word] = 1    \n",
    "    return freqs\n",
    "\n",
    "\n",
    "\n",
    "sentence = \"String should be tokenized. String must be stemmed. String must not have punctuations\"\n",
    "\n",
    "tokens = wordTockenize(sentence)\n",
    "tokens = removePunctandStopWords(tokens)\n",
    "tokens = stem(tokens)\n",
    "\n",
    "freq = build_freqs(tokens)\n",
    "\n",
    "freq.items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "postiveSentences = [\"I am happy\", \"Happiness is bliss\", \"I enjoy life\", \"I enjoy learning machine learning\", \"I like to build machine learning models\", \"I like to play basketball\", \"I like my colleagues at FEV\", \"It was a great day\", \"Live your life for the fullest\", \"I love my roomates\", \"I like something\", \"I like footaball\"]\n",
    "\n",
    "negativeSentences = [\"I am sad\", \"I dont not like\", \"I hate to\", \"I a worried\", \"It is slow\", \"We should stop\", 'I dont like machine learning', \"I give up\", \"I hate it\", \"Sadness is not good\", \"He is upset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = np.ones((10,1))\n",
    "yn = np.zeros((10,1))\n",
    "\n",
    "def buildData(listVal):\n",
    "    all = []\n",
    "\n",
    "    if type(listVal) is list:\n",
    "        for data in listVal:\n",
    "            tokens = wordTockenize(data)\n",
    "            tokens = removePunctandStopWords(tokens)\n",
    "            tokens = stem(tokens)\n",
    "            all += tokens\n",
    "    else:\n",
    "        tokens = wordTockenize(listVal)\n",
    "        tokens = removePunctandStopWords(tokens)\n",
    "        tokens = stem(tokens)\n",
    "        all += tokens\n",
    "\n",
    "    return all\n",
    "\n",
    "\n",
    "\n",
    "def buildFreqTable(tokens, y):\n",
    "    dict = defaultdict()\n",
    "    freq = build_freqs(tokens)\n",
    "    for key in freq.keys():\n",
    "        val = (key, y)\n",
    "        dict[val] = freq[key]\n",
    "\n",
    "    return dict\n",
    "\n",
    "def extractFeatures(words, freq):\n",
    "    x  = np.zeros(3)\n",
    "\n",
    "    # bias term is set to 1\n",
    "    x[0] = 1 \n",
    "\n",
    " # loop through each word in the list of words\n",
    "    for word in words:\n",
    "        \n",
    "        if (word, 1.0) in freq:\n",
    "            # increment the word count for the positive label 1\n",
    "            x[1] += freq.get((word, 1.0))\n",
    "        else:\n",
    "            x[1] += 0\n",
    "            \n",
    "        if (word, 0.0) in freq:\n",
    "            # increment the word count for the positive label 0\n",
    "            x[2] += freq.get((word, 0.0))\n",
    "        else:\n",
    "            x[2] += 0\n",
    "        \n",
    "    x = x[None, :]  # adding batch dimension for further processing\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildDataPos = buildData(postiveSentences)\n",
    "buildDataNeg = buildData(negativeSentences)\n",
    "\n",
    "labelledDataPos = buildFreqTable(buildDataPos, 1.0)\n",
    "labelledDataNeg = buildFreqTable(buildDataNeg, 0.0)\n",
    "\n",
    "\n",
    "allWords = buildDataPos + buildDataNeg\n",
    "allDictword = merged_dict = defaultdict(int, dict(Counter(labelledDataPos) + Counter(labelledDataNeg)))\n",
    "\n",
    "\n",
    "train_x = postiveSentences + negativeSentences\n",
    "train_y = np.concatenate((np.ones(len(postiveSentences)), np.zeros(len(negativeSentences))), axis=0).reshape(-1, 1)\n",
    "tmp = extractFeatures(allWords, allDictword)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (fc3): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(3, 4)  # Fully connected layer with 3 input neurons and 4 hidden neurons\n",
    "        self.relu = nn.ReLU()       # Activation function (ReLU)\n",
    "        self.fc2 = nn.Linear(4, 4)  # Fully connected layer with 4 hidden neurons and 4 output neurons\n",
    "        self.fc3 = nn.Linear(4, 1)  # Fully connected layer with 4 hidden neurons and 4 output neurons\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Define a custom loss function (MSE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer (e.g., stochastic gradient descent)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 1, 2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# We can shuffle the data if we wish to\n",
    "\n",
    "# Create a PyTorch tensor\n",
    "original_tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# Get the number of elements in the tensor\n",
    "num_elements = original_tensor.size(0)\n",
    "\n",
    "# Generate a random permutation of indices\n",
    "random_indices = torch.randperm(num_elements)\n",
    "\n",
    "# Use the random indices to shuffle the tensor\n",
    "shuffled_tensor = original_tensor[random_indices]\n",
    "\n",
    "# Print the shuffled tensor\n",
    "print(shuffled_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Value of input X to train the model\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extractFeatures(buildData(train_x[i]), allDictword)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 0.2219\n",
      "Epoch [200/10000], Loss: 0.2844\n",
      "Epoch [300/10000], Loss: 0.2288\n",
      "Epoch [400/10000], Loss: 0.2155\n",
      "Epoch [500/10000], Loss: 0.2782\n",
      "Epoch [600/10000], Loss: 0.2250\n",
      "Epoch [700/10000], Loss: 0.2218\n",
      "Epoch [800/10000], Loss: 0.2683\n",
      "Epoch [900/10000], Loss: 0.2201\n",
      "Epoch [1000/10000], Loss: 0.1820\n",
      "Epoch [1100/10000], Loss: 0.4264\n",
      "Epoch [1200/10000], Loss: 0.1221\n",
      "Epoch [1300/10000], Loss: 0.1371\n",
      "Epoch [1400/10000], Loss: 0.2189\n",
      "Epoch [1500/10000], Loss: 0.0381\n",
      "Epoch [1600/10000], Loss: 0.2083\n",
      "Epoch [1700/10000], Loss: 0.1752\n",
      "Epoch [1800/10000], Loss: 0.0455\n",
      "Epoch [1900/10000], Loss: 0.5995\n",
      "Epoch [2000/10000], Loss: 0.1157\n",
      "Epoch [2100/10000], Loss: 0.0164\n",
      "Epoch [2200/10000], Loss: 0.0825\n",
      "Epoch [2300/10000], Loss: 0.0714\n",
      "Epoch [2400/10000], Loss: 0.1250\n",
      "Epoch [2500/10000], Loss: 0.0516\n",
      "Epoch [2600/10000], Loss: 0.0982\n",
      "Epoch [2700/10000], Loss: 0.0217\n",
      "Epoch [2800/10000], Loss: 0.0301\n",
      "Epoch [2900/10000], Loss: 0.0369\n",
      "Epoch [3000/10000], Loss: 0.0684\n",
      "Epoch [3100/10000], Loss: 0.0190\n",
      "Epoch [3200/10000], Loss: 0.0111\n",
      "Epoch [3300/10000], Loss: 0.0016\n",
      "Epoch [3400/10000], Loss: 0.9877\n",
      "Epoch [3500/10000], Loss: 0.0000\n",
      "Epoch [3600/10000], Loss: 0.0012\n",
      "Epoch [3700/10000], Loss: 0.0096\n",
      "Epoch [3800/10000], Loss: 0.0000\n",
      "Epoch [3900/10000], Loss: 0.0039\n",
      "Epoch [4000/10000], Loss: 0.0033\n",
      "Epoch [4100/10000], Loss: 0.0004\n",
      "Epoch [4200/10000], Loss: 0.8325\n",
      "Epoch [4300/10000], Loss: 0.0010\n",
      "Epoch [4400/10000], Loss: 0.0005\n",
      "Epoch [4500/10000], Loss: 0.0019\n",
      "Epoch [4600/10000], Loss: 0.0046\n",
      "Epoch [4700/10000], Loss: 0.0775\n",
      "Epoch [4800/10000], Loss: 0.0030\n",
      "Epoch [4900/10000], Loss: 0.1404\n",
      "Epoch [5000/10000], Loss: 0.0083\n",
      "Epoch [5100/10000], Loss: 0.0024\n",
      "Epoch [5200/10000], Loss: 0.0111\n",
      "Epoch [5300/10000], Loss: 0.0403\n",
      "Epoch [5400/10000], Loss: 0.0021\n",
      "Epoch [5500/10000], Loss: 0.0018\n",
      "Epoch [5600/10000], Loss: 0.0058\n",
      "Epoch [5700/10000], Loss: 0.0169\n",
      "Epoch [5800/10000], Loss: 0.0000\n",
      "Epoch [5900/10000], Loss: 0.0038\n",
      "Epoch [6000/10000], Loss: 0.0016\n",
      "Epoch [6100/10000], Loss: 0.0000\n",
      "Epoch [6200/10000], Loss: 0.0001\n",
      "Epoch [6300/10000], Loss: 0.0001\n",
      "Epoch [6400/10000], Loss: 0.0003\n",
      "Epoch [6500/10000], Loss: 0.0001\n",
      "Epoch [6600/10000], Loss: 0.0000\n",
      "Epoch [6700/10000], Loss: 0.0002\n",
      "Epoch [6800/10000], Loss: 0.0001\n",
      "Epoch [6900/10000], Loss: 0.0012\n",
      "Epoch [7000/10000], Loss: 0.0059\n",
      "Epoch [7100/10000], Loss: 0.0011\n",
      "Epoch [7200/10000], Loss: 0.0053\n",
      "Epoch [7300/10000], Loss: 0.0002\n",
      "Epoch [7400/10000], Loss: 0.0010\n",
      "Epoch [7500/10000], Loss: 0.0008\n",
      "Epoch [7600/10000], Loss: 0.0044\n",
      "Epoch [7700/10000], Loss: 0.0009\n",
      "Epoch [7800/10000], Loss: 0.0001\n",
      "Epoch [7900/10000], Loss: 0.0016\n",
      "Epoch [8000/10000], Loss: 0.0026\n",
      "Epoch [8100/10000], Loss: 0.0000\n",
      "Epoch [8200/10000], Loss: 0.0014\n",
      "Epoch [8300/10000], Loss: 0.0008\n",
      "Epoch [8400/10000], Loss: 0.0000\n",
      "Epoch [8500/10000], Loss: 0.0000\n",
      "Epoch [8600/10000], Loss: 0.0000\n",
      "Epoch [8700/10000], Loss: 0.0000\n",
      "Epoch [8800/10000], Loss: 0.0000\n",
      "Epoch [8900/10000], Loss: 0.0000\n",
      "Epoch [9000/10000], Loss: 0.0000\n",
      "Epoch [9100/10000], Loss: 0.0000\n",
      "Epoch [9200/10000], Loss: 0.0007\n",
      "Epoch [9300/10000], Loss: 0.0024\n",
      "Epoch [9400/10000], Loss: 0.0007\n",
      "Epoch [9500/10000], Loss: 0.0023\n",
      "Epoch [9600/10000], Loss: 0.0001\n",
      "Epoch [9700/10000], Loss: 0.0006\n",
      "Epoch [9800/10000], Loss: 0.0003\n",
      "Epoch [9900/10000], Loss: 0.0021\n",
      "Epoch [10000/10000], Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "# Training the data Model\n",
    "def trainingModel(X, Y):\n",
    "    # Training loop\n",
    "    num_epochs = 10000\n",
    "    index = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        index = index % len(train_x)\n",
    "        \n",
    "        # i = i % len(train_x)\n",
    "        input_data = torch.tensor(X[index], dtype=torch.float32)\n",
    "        target = torch.tensor(Y[index], dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(input_data)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(predictions, target)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print the loss for monitoring\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "trainingModel(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8339]], grad_fn=<SigmoidBackward0>)\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_sentence = 'i like'\n",
    "\n",
    "x_calc = extractFeatures(buildData(my_sentence), allDictword)\n",
    "y_hat = model(torch.tensor(x_calc, dtype=torch.float32))\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
